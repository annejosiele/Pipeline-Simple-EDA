#Pipeline-Simple-EDA


# Importing libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew, kurtosis, boxcox, yeojohnson
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, mutual_info_regression

# Loading and cleaning data
data = pd.read_excel("your-dataset.xlsx")

# Missing values treatment
imputer = SimpleImputer(strategy='mean')
for col in data.select_dtypes(include=[np.number]).columns:
    data[col] = imputer.fit_transform(data[[col]])

# Checking data consistency
print(data.info())

# Univariate analysis
for col in data.select_dtypes(include=[np.number]).columns:
    plt.figure()
    sns.histplot(data[col], kde=True, bins=30)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Descriptive measures
summary = data.select_dtypes(include=[np.number]).describe().T
summary['skew'] = data.select_dtypes(include=[np.number]).apply(lambda x: skew(x.dropna()))
summary['kurtosis'] = data.select_dtypes(include=[np.number]).apply(lambda x: kurtosis(x.dropna()))
print(summary)

# Bivariate analysis
correlation_matrix = data.select_dtypes(include=[np.number]).corr()
plt.figure(figsize=(20, 17))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Variable Correlation Matrix')
plt.show()

# Treating high correlation
threshold_corr = 0.9
to_remove = set()
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold_corr:
            colname = correlation_matrix.columns[i]
            to_remove.add(colname)

# Refresh
data_reduced = data.drop(columns=to_remove)
print(f"Variáveis removidas devido à alta correlação: {to_remove}")


# Data visualization
for col in data_reduced.select_dtypes(include=[np.number]).columns:
    plt.figure()
    sns.boxplot(x=data_reduced[col])
    plt.title(f'Boxplot of {col}')
    plt.xlabel(col)
    plt.show()

# Nonlinear relationships using mutual information
print("\ncalculating Mutual Information...")
X = data_reduced.drop(columns=["your-target-variable"], errors='ignore')
y = data_reduced["target-variable"] if "target-variable" in data_reduced.columns else None

if y is not None:
    mi_scores = mutual_info_regression(X, y, random_state=42)
    mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)
    print("\nScore of Mutual Information :")
    print(mi_scores)

    # Visualizar MI
    plt.figure(figsize=(15, 15))
    sns.barplot(x=mi_scores.values, y=mi_scores.index, palette='plasma')
    plt.title("Mutual Information between Variables and Your Target Variable")
    plt.xlabel("Mutual Information")
    plt.ylabel("Variables")
    plt.show()


# Identify and remove variables with low variance
threshold_variance = 0.01
selector = VarianceThreshold(threshold=threshold_variance)
selector.fit(data_reduced.select_dtypes(include=[np.number]))
columns_kept = data_reduced.select_dtypes(include=[np.number]).columns[selector.get_support()]
data_final = data_reduced[columns_kept]

print(f"Variables removed due to low variance: {set(data_reduced.columns) - set(columns_kept)}")

# Final data
print("Final dataset, ready to modeling:")
print(data_final.head())


# Saving the dataset
data_final.to_excel('name-of-final-dataset.xlsx', index=False)
